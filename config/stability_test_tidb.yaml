generateName: stability-test-tidb-
resourcePool: quantify_products
reclaimPolicy:
  gcStrategy:
    onFailure:
      artifact:
        monitor: true
        log: true
    onSuccess:
      artifact:
        monitor: true
        log: true

items:
  - name: stability-test # 资源对象项名称
    type: TIDB_CLUSTER # 资源对象类型
    spec:
      # 如果使用一个具体的版本, 例如 v5.0.1 而不是 nightly, master 等, 通过设置 IfNotPresent 可以加速创建速度
      imagePullPolicy: IfNotPresent
      pd:
        replicas: 1
        storageClassName: fast-disks
        image: pingcap/pd:v5.3.0
        # more refer to: https://github.com/tikv/pd/blob/master/conf/config.toml
        # config is string type and toml format, I prefer uses https://www.convertsimple.com/convert-yaml-to-toml/ to transfer yaml to toml
        requests:
          cpu: 4000m
          memory: 8Gi
          storage: 500Gi
        limits:
          cpu: 4000m
          memory: 8Gi
          storage: 500Gi
        service:
          type: NodePort
        config: |
          [log]
          level = "info"
      tidb:
        replicas: 1
        storageClassName: fast-disks
        image: pingcap/tidb:v5.3.0
        requests:
          cpu: 8000m
          memory: 32Gi
          storage: 500Gi
        limits:
          cpu: 8000m
          memory: 32Gi
          storage: 500Gi
        service:
          type: NodePort
        securityContext:
          capabilities:
            add: ["SYS_ADMIN", "SYS_PTRACE", "NET_ADMIN"]
        config: |
          [log]
          # Log level: debug, info, warn, error, fatal.
          level = "info"

      tikv:
        replicas: 4
        storageClassName: fast-disks
        image: pingcap/tikv:v5.3.0
        # more refer to: https://github.com/tikv/tikv/blob/master/etc/config-template.toml
        requests:
          cpu: 8000m
          memory: 32Gi
          storage: 2000Gi
        limits:
          cpu: 8000m
          memory: 32Gi
          storage: 2000Gi
        service:
          type: NodePort
        config: |
          ## Log levels: trace, debug, info, warning, error, critical.
          ## Note that `debug` and `trace` are only available in development builds.
          [storage]
          reserve-space = 0
          [storage.block-cache]
          shared = true
          capacity = "12G"
          [rocksdb.defaultcf]
          compression-per-level = ["no", "no", "no", "no", "no", "no", "no"]
          [rocksdb.writecf]
          compression-per-level = ["no", "no", "no", "no", "no", "no", "no"]
          [rocksdb.lockcf]
          compression-per-level = ["no", "no", "no", "no", "no", "no", "no"]
          [raftstore]
          apply-low-priority-pool-size = 4
          [server]
          raft-client-queue-size = 81920

      tiflash:
        replicas: 4
        storageClassName: fast-disks
        image: pingcap/tiflash:v5.3.0
        # more refer to: https://docs.pingcap.com/zh/tidb/stable/tiflash-configuration#%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6-tiflashtoml
        # you need at lease uses github.com/pingcap/test-infra/sdk@v0.0.0-20210705141439-f1670c1d3048 or newer version
        requests:
          cpu: 32000m
          memory: 128Gi
        limits:
          cpu: 32000m
          memory: 128Gi
        storageClaims:
          - resources:
              requests:
                storage: 1000Gi
            storageClassName: fast-disks
        service:
          type: NodePort
        securityContext:
          capabilities:
            add: ["SYS_ADMIN", "SYS_PTRACE", "NET_ADMIN"]
        config:
          config: |
            [log]
            level = "info"
            [profiles.default]
            max_memory_usage = 128849018880
            max_memory_usage_for_all_queries = 128849018880
            dt_segment_force_merge_delta_rows = 8000000
            dt_segment_force_merge_delta_size = 1073741824
            dt_segment_stop_write_delta_rows = 40000000
            dt_enable_logical_split = false
          proxy: |
            log-level = "info"
            log-file = "/var/lib/tiflash/log/proxy.log"
            [raftstore]
            apply-low-priority-pool-size = 16
            apply-pool-size = 4
            store-pool-size = 4
            snap-handle-pool-size = 8
            [server]
            raft-client-queue-size = 81920

  - name: workload
    type: WORKLOAD_NODE
    spec:
      container:
        name: workload
        image: hub-new.pingcap.net/qa/betting-workload
        securityContext:
          capabilities:
            add: [ "SYS_ADMIN", "SYS_PTRACE", "NET_ADMIN" ]
        resources:
          requests:
            memory: 16Gi
            cpu: 8000m
